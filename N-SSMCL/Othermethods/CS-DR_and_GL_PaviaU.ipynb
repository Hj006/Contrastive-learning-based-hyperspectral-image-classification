{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import skimage.exposure\n",
    "import warnings\n",
    "from io import StringIO\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.segmentation\n",
    "import random\n",
    "from scipy.sparse import coo_array\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 如果你显示中文，改为你系统支持的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load__with_full_test(data_path: Path, candidate_counts: dict, num_train_per_class=10, seed=42):\n",
    "    \"\"\"\n",
    "    加载 Pavia University 数据集：\n",
    "    - 从每类中抽取固定数量作为候选样本\n",
    "    - 从候选中抽取 10 个作为训练样本\n",
    "    - 所有 ground truth 像素都作为测试样本（包括候选/训练区域）\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    pavia_data = loadmat(data_path / 'PaviaU.mat')['paviaU']\n",
    "    gt = loadmat(data_path / 'PaviaU_gt.mat')['paviaU_gt']\n",
    "    h, w, c = pavia_data.shape\n",
    "\n",
    "    label_dict = {\n",
    "        1: 'Asphalt road', 2: 'Meadows', 3: 'Gravel',\n",
    "        4: 'Trees', 5: 'Painted metal sheets', 6: 'Bare Soil',\n",
    "        7: 'Bitumen', 8: 'Self-Blocking Bricks', 9: 'Shadows',\n",
    "    }\n",
    "\n",
    "    train_rows, train_cols, train_data = [], [], []\n",
    "    candidate_rows, candidate_cols, candidate_data = [], [], []\n",
    "\n",
    "    for label, cand_count in candidate_counts.items():\n",
    "        coords = np.argwhere(gt == label)\n",
    "        if len(coords) < cand_count:\n",
    "            raise ValueError(f\" 类 {label} 样本不足，只有 {len(coords)}，无法抽取 {cand_count} 个候选样本\")\n",
    "        np.random.shuffle(coords)\n",
    "\n",
    "        candidate_coords = coords[:cand_count]\n",
    "        train_coords = candidate_coords[:num_train_per_class]\n",
    "\n",
    "        # 训练样本\n",
    "        for r, c in train_coords:\n",
    "            train_rows.append(r)\n",
    "            train_cols.append(c)\n",
    "            train_data.append(label)\n",
    "\n",
    "        # 候选样本（包含训练）\n",
    "        for r, c in candidate_coords:\n",
    "            candidate_rows.append(r)\n",
    "            candidate_cols.append(c)\n",
    "            candidate_data.append(label)\n",
    "\n",
    "    # 构建稀疏矩阵\n",
    "    train_truth = coo_matrix((train_data, (train_rows, train_cols)), shape=(h, w), dtype=int)\n",
    "    candidate_truth = coo_matrix((candidate_data, (candidate_rows, candidate_cols)), shape=(h, w), dtype=int)\n",
    "\n",
    "    #  测试集直接包含所有 ground truth\n",
    "    test_rows, test_cols = np.where(gt > 0)\n",
    "    test_data = gt[test_rows, test_cols]\n",
    "    test_truth = coo_matrix((test_data, (test_rows, test_cols)), shape=(h, w), dtype=int)\n",
    "\n",
    "    info = {\n",
    "        'n_band': c,\n",
    "        'width': w,\n",
    "        'height': h,\n",
    "        'label_dict': label_dict\n",
    "    }\n",
    "\n",
    "    return pavia_data.transpose(2, 0, 1), train_truth, candidate_truth, test_truth, info\n",
    "\n",
    "\n",
    "def merge_train_test(train_truth, test_truth, shape):\n",
    "    \"\"\"\n",
    "    合并训练集和测试集稀疏矩阵为一个新的训练集矩阵。\n",
    "    \n",
    "    参数:\n",
    "        train_truth: coo_matrix, 原始训练集稀疏矩阵\n",
    "        test_truth: coo_matrix, 原始测试集稀疏矩阵\n",
    "        shape: tuple, 数据的形状 (height, width)\n",
    "        \n",
    "    返回:\n",
    "        merged_truth: coo_matrix, 合并后的训练集稀疏矩阵\n",
    "    \"\"\"\n",
    "    # 合并行、列和数据\n",
    "    merged_rows = np.concatenate([train_truth.row, test_truth.row])\n",
    "    merged_cols = np.concatenate([train_truth.col, test_truth.col])\n",
    "    merged_data = np.concatenate([train_truth.data, test_truth.data])\n",
    "    \n",
    "    # 创建新的稀疏矩阵\n",
    "    merged_truth = coo_matrix((merged_data, (merged_rows, merged_cols)), shape=shape)\n",
    "    return merged_truth\n",
    "\n",
    "def apply_pca_train_only(hsi_data, train_truth, num_components=40):\n",
    "    \"\"\"\n",
    "    使用训练区域的光谱数据训练 PCA 模型，并应用到整个数据集。\n",
    "    \n",
    "    参数:\n",
    "        hsi_data: numpy.ndarray, 高光谱图像数据, 形状为 [C, H, W]\n",
    "        train_truth: coo_array, 训练区域的稀疏矩阵，表示训练样本的位置\n",
    "        num_components: int, 保留的主成分数量\n",
    "        \n",
    "    返回:\n",
    "        pca_data: numpy.ndarray, PCA 降维后的数据，形状为 [num_components, H, W]\n",
    "        explained_variance_ratio: PCA 的累计解释方差比\n",
    "    \"\"\"\n",
    "    c, h, w = hsi_data.shape  # 高光谱数据的形状\n",
    "    rows, cols = train_truth.row, train_truth.col  # 提取训练区域的行列索引\n",
    "    \n",
    "    # 提取训练区域的光谱数据 [num_samples, num_channels]\n",
    "    train_spectra = hsi_data[:, rows, cols].T  # 转置为 [num_samples, num_channels]\n",
    "    \n",
    "    # 在训练区域数据上拟合 PCA\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(train_spectra)  # 仅在训练区域数据上训练 PCA\n",
    "    \n",
    "    # 转换整个数据集 [C, H, W] -> [H×W, C]\n",
    "    reshaped_data = hsi_data.reshape(c, -1).T  # [H×W, C]\n",
    "    reduced_data = pca.transform(reshaped_data)  # 降维 [H×W, num_components]\n",
    "    \n",
    "    # 恢复为原始图像的形状 [num_components, H, W]\n",
    "    pca_data = reduced_data.T.reshape(num_components, h, w)  # [num_components, H, W]\n",
    "    \n",
    "    return pca_data, pca.explained_variance_ratio_\n",
    "\n",
    "def superpixel_segmentation(hsi_data, num_superpixels=100):\n",
    "    \"\"\"\n",
    "    使用 SLIC 超像素分割对 HSI 进行分割。\n",
    "\n",
    "    参数:\n",
    "        hsi_data: numpy.ndarray, 形状为 (C, H, W)\n",
    "        num_superpixels: 生成的超像素数量\n",
    "        \n",
    "    返回:\n",
    "        labels: 超像素标签矩阵，形状为 (H, W)\n",
    "    \"\"\"\n",
    "    # 先用 PCA 提取第一主成分\n",
    "    first_pc = PCA(n_components=1).fit_transform(hsi_data.reshape(hsi_data.shape[0], -1).T)\n",
    "    first_pc = first_pc.reshape(hsi_data.shape[1:])  # 变成 (H, W)\n",
    "\n",
    "    # 修正错误：复制 3 通道，使其符合 SLIC 需 \n",
    "    first_pc_rgb = np.stack([first_pc] * 3, axis=-1)  # 变成 (H, W, 3)\n",
    "\n",
    "    # 正确调用 skimage.segmentation.sli \n",
    "    labels = skimage.segmentation.slic(first_pc_rgb, n_segments=num_superpixels, compactness=10, start_label=0, channel_axis=-1)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def compute_superpixel_feature_map(hsi_data, superpixel_labels):\n",
    "    \"\"\"\n",
    "    对每个超像素提取均值特征（不降维），并生成特征图。\n",
    "    \"\"\"\n",
    "    h, w = superpixel_labels.shape\n",
    "    c = hsi_data.shape[0]\n",
    "    superpixel_feature_map = np.zeros((c, h, w), dtype=np.float32)\n",
    "    superpixel_feature_dict = {}\n",
    "\n",
    "    unique_labels = np.unique(superpixel_labels)\n",
    "    for label in unique_labels:\n",
    "        mask = (superpixel_labels == label)\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "        pixels = hsi_data[:, mask].T  # (N, C)\n",
    "        mean_feat = np.mean(pixels, axis=0)  # (C,)\n",
    "        superpixel_feature_dict[label] = mean_feat\n",
    "        for i in range(c):\n",
    "            superpixel_feature_map[i][mask] = mean_feat[i]\n",
    "\n",
    "    return superpixel_feature_map, superpixel_feature_dict\n",
    "\n",
    "\n",
    "\n",
    "def compute_superpixel_pca(hsi_data, superpixel_labels, merged_train_truth, num_components=20):\n",
    "    \"\"\"\n",
    "    仅在训练区域计算每个超像素的局部 PCA，并返回降维后的特征。\n",
    "\n",
    "    参数:\n",
    "        hsi_data: numpy.ndarray, 形状为 (C, H, W)\n",
    "        superpixel_labels: numpy.ndarray, 形状为 (H, W)\n",
    "        merged_train_truth: coo_matrix, 训练区域掩码\n",
    "        num_components: int, PCA 维度\n",
    "        \n",
    "    返回:\n",
    "        superpixel_pca_map: numpy.ndarray, 形状为 (num_components, H, W)\n",
    "        superpixel_pca_dict: dict, 每个超像素的局部PCA结果\n",
    "    \"\"\"\n",
    "    h, w = superpixel_labels.shape\n",
    "    c = hsi_data.shape[0]\n",
    "    superpixel_pca_map = np.zeros((num_components, h, w))\n",
    "    superpixel_pca_dict = {}\n",
    "\n",
    "    # 获取训练区域坐标\n",
    "    mask_train = merged_train_truth.toarray() > 0\n",
    "\n",
    "    unique_labels = np.unique(superpixel_labels)\n",
    "    for label in unique_labels:\n",
    "        mask = (superpixel_labels == label)\n",
    "\n",
    "        # 当前超像素区域内的训练区域\n",
    "        train_mask = np.logical_and(mask, mask_train)\n",
    "        if np.sum(train_mask) == 0:\n",
    "            continue\n",
    "\n",
    "        pixels = hsi_data[:, train_mask].T  # shape: (num_samples, num_channels)\n",
    "\n",
    "        # 如果样本数不足，跳过\n",
    "        if pixels.shape[0] <= num_components:\n",
    "            continue\n",
    "\n",
    "        # 执行局部 PCA\n",
    "        pca = PCA(n_components=num_components)\n",
    "        reduced_pixels = pca.fit_transform(pixels)\n",
    "        superpixel_pca_dict[label] = reduced_pixels.T  # shape: (num_components, N)\n",
    "\n",
    "        # 计算均值写入整块区域\n",
    "        for i in range(num_components):\n",
    "            superpixel_pca_map[i, mask] = np.mean(reduced_pixels[:, i])\n",
    "\n",
    "    return superpixel_pca_map, superpixel_pca_dict\n",
    "\n",
    "\n",
    "\n",
    "def split_cube(hsi_cube):\n",
    "    \"\"\"\n",
    "    将高光谱立方块沿通道维度均匀切分。\n",
    "    参数:\n",
    "        hsi_cube: torch.Tensor, 形状为 [H, W, C]\n",
    "    返回:\n",
    "        hsi_cube_a, hsi_cube_b: 两个子立方块\n",
    "    \"\"\"\n",
    "    _, _, c = hsi_cube.shape\n",
    "    c1 = c // 2  # 每个子块保留一半的通道\n",
    "    return hsi_cube[:, :, :c1], hsi_cube[:, :, c1:]\n",
    "\n",
    "def extract_cube(data, x, y, size):\n",
    "    \"\"\"\n",
    "    从高光谱数据中提取局部立方块，并在边界不足时进行填充。\n",
    "    \n",
    "    参数:\n",
    "        data: numpy.ndarray, 形状为 [C, H, W]\n",
    "        x, y: int, 立方块的中心像素坐标\n",
    "        size: tuple, 立方块的大小 (s, s)，要求 s 必须是奇数\n",
    "    \n",
    "    返回:\n",
    "        cube: numpy.ndarray, 形状为 [C, s, s]\n",
    "    \"\"\"\n",
    "    assert size[0] % 2 == 1, \"立方块大小必须是奇数，以确保中心点对齐。\"\n",
    "    \n",
    "    c, h, w = data.shape\n",
    "    half_size = size[0] // 2  # 计算半径\n",
    "\n",
    "    # 计算提取区域的坐标范围\n",
    "    x_min, x_max = max(0, x - half_size), min(h, x + half_size + 1)\n",
    "    y_min, y_max = max(0, y - half_size), min(w, y + half_size + 1)\n",
    "\n",
    "    # 提取局部数据\n",
    "    cube = data[:, x_min:x_max, y_min:y_max]\n",
    "\n",
    "    # 计算需要填充的大小\n",
    "    pad_x_min, pad_x_max = max(0, half_size - x), max(0, x + half_size + 1 - h)\n",
    "    pad_y_min, pad_y_max = max(0, half_size - y), max(0, y + half_size + 1 - w)\n",
    "\n",
    "    # 使用边缘填充，确保输出形状为 (C, size, size)\n",
    "    pad_width = [\n",
    "        (0, 0),  # 不填充通道维度\n",
    "        (pad_x_min, pad_x_max),  # 高度填充\n",
    "        (pad_y_min, pad_y_max),  # 宽度填充\n",
    "    ]\n",
    "    cube = np.pad(cube, pad_width, mode=\"edge\")  # 填充值是边界像素，而不是反射\n",
    "    #cube = np.pad(cube, pad_width, mode=\"reflect\")\n",
    "    return cube\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=40):\n",
    "        \"\"\"\n",
    "        特征提取网络。\n",
    "        参数:\n",
    "            input_channels: 输入的通道数，例如 20。\n",
    "        \"\"\"\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)  # 第一层卷积\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 第二层卷积\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # 第三层卷积\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 最大池化层\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))  # 卷积 + 批归一化 + 激活 + 池化\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        return x.view(x.size(0), -1)  # 展平特征\n",
    "    \n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=128, output_dim=8):\n",
    "        \"\"\"\n",
    "        特征投影模块。\n",
    "        参数:\n",
    "            input_dim: 输入特征的维度，例如 128。\n",
    "            output_dim: 投影后的维度，例如 8。\n",
    "        \"\"\"\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.fc(x), dim=1)  # 投影后的特征归一化\n",
    "\n",
    "\n",
    "def contrastive_loss(features_a, features_b, temperature=1.0):\n",
    "    \"\"\"\n",
    "    计算对比损失。\n",
    "    参数:\n",
    "        features_a, features_b: 投影后的特征，形状为 [batch_size, projection_dim]\n",
    "        temperature: 温度系数\n",
    "    返回:\n",
    "        loss: 对比损失值\n",
    "    \"\"\"\n",
    "    batch_size = features_a.size(0)\n",
    "    features = torch.cat([features_a, features_b], dim=0)  # 拼接特征\n",
    "    sim_matrix = F.cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim=2)  # 计算相似度\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    # 构造标签\n",
    "    labels = torch.arange(batch_size, device=features_a.device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "    # 使用交叉熵损失\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss\n",
    "\n",
    "def extract_precise_superpixel_pca(superpixel_pca_dict, superpixel_labels, x, y):\n",
    "    \"\"\"\n",
    "    获取像素 (x, y) 所属超像素的 SuperPCA 特征。\n",
    "    \n",
    "    参数:\n",
    "        superpixel_pca_dict: 字典，存储每个超像素的 PCA 结果\n",
    "        superpixel_labels: 超像素标签矩阵\n",
    "        x, y: 目标像素坐标\n",
    "        \n",
    "    返回:\n",
    "        superpixel_features: numpy.ndarray, 形状为 (20,)\n",
    "    \"\"\"\n",
    "    superpixel_label = superpixel_labels[x, y]  # 获取该像素的超像素标签\n",
    "    \n",
    "    # 获取该超像素块的所有 PCA 结果\n",
    "    superpixel_features = superpixel_pca_dict[superpixel_label]  # 形状为 (20, N)，N 是该超像素块内像素数\n",
    "\n",
    "    # 取所有像素的平均值，确保返回 20 维的特征向量\n",
    "    superpixel_features = np.mean(superpixel_features, axis=1)  # 形状变为 (20,)\n",
    "\n",
    "    return superpixel_features\n",
    "\n",
    "\n",
    "\n",
    "class S3PCADataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 pca_data, \n",
    "                 superpixel_pca_map, \n",
    "                 superpixel_pca_dict, \n",
    "                 superpixel_labels, \n",
    "                 global_pca, \n",
    "                 patch_size=11, \n",
    "                 num_samples=1000,\n",
    "                 mode=\"original\"  #  加一个模式选择参数\n",
    "                ):\n",
    "        self.pca_data = pca_data\n",
    "        self.superpixel_pca_map = superpixel_pca_map\n",
    "        self.superpixel_pca_dict = superpixel_pca_dict\n",
    "        self.superpixel_labels = superpixel_labels\n",
    "        self.global_pca = global_pca\n",
    "        self.patch_size = patch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.mode = mode  #  保存模式\n",
    "        self.h, self.w = pca_data.shape[1], pca_data.shape[2]\n",
    "\n",
    "        #  构造 valid_coords 列表\n",
    "        self.valid_coords = [\n",
    "            (x, y)\n",
    "            for x in range(patch_size // 2, self.h - patch_size // 2)\n",
    "            for y in range(patch_size // 2, self.w - patch_size // 2)\n",
    "            if superpixel_labels[x, y] in superpixel_pca_dict\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.num_samples, len(self.valid_coords))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.valid_coords[idx % len(self.valid_coords)]\n",
    "\n",
    "        if self.mode == \"raw_only\":\n",
    "            raw_patch = extract_cube(self.pca_data, x, y, (self.patch_size, self.patch_size))\n",
    "            raw_patch = torch.tensor(raw_patch, dtype=torch.float32)\n",
    "            half = raw_patch.shape[0] // 2\n",
    "            cube_a, cube_b = torch.split(raw_patch[:2*half], [half, half], dim=0)\n",
    "            return cube_a, cube_b\n",
    "\n",
    "        elif self.mode == \"local_global\":\n",
    "            superpixel_feat = self.superpixel_pca_dict[self.superpixel_labels[x, y]]  # shape (103,)\n",
    "            superpixel_patch = torch.tensor(superpixel_feat[:, None, None], dtype=torch.float32)\n",
    "            superpixel_patch = superpixel_patch.expand(-1, self.patch_size, self.patch_size)\n",
    "\n",
    "            global_patch = extract_cube(self.global_pca, x, y, (self.patch_size, self.patch_size)).astype(np.float32)\n",
    "            global_patch = torch.tensor(global_patch, dtype=torch.float32)\n",
    "\n",
    "            return superpixel_patch, global_patch\n",
    "\n",
    "\n",
    "        elif self.mode == \"pca_super_rawglobal\":\n",
    "            raw_patch = extract_cube(self.pca_data, x, y, (self.patch_size, self.patch_size))\n",
    "            raw_patch = torch.tensor(raw_patch, dtype=torch.float32)\n",
    "            total_channels = raw_patch.shape[0]\n",
    "            half = total_channels // 2\n",
    "            raw_a, raw_b = torch.split(raw_patch[:2*half], [half, half], dim=0)\n",
    "\n",
    "            superpixel_patch = extract_precise_superpixel_pca(self.superpixel_pca_dict, self.superpixel_labels, x, y)\n",
    "            superpixel_patch = torch.tensor(superpixel_patch[:, None, None], dtype=torch.float32)\n",
    "            superpixel_patch = superpixel_patch.expand(-1, self.patch_size, self.patch_size)\n",
    "\n",
    "            global_patch = extract_cube(self.global_pca, x, y, (self.patch_size, self.patch_size))\n",
    "            global_patch = torch.tensor(global_patch, dtype=torch.float32)\n",
    "\n",
    "            cube_a = torch.cat([raw_a, superpixel_patch], dim=0)\n",
    "            cube_b = torch.cat([raw_b, global_patch], dim=0)\n",
    "            return cube_a, cube_b\n",
    "\n",
    "        else:  # 默认原始方式\n",
    "            pca_patch = extract_cube(self.pca_data, x, y, (self.patch_size, self.patch_size))\n",
    "            pca_patch = torch.tensor(pca_patch, dtype=torch.float32)\n",
    "            pca_channels = pca_patch.shape[0]\n",
    "            half_channels = pca_channels // 2\n",
    "            pca_patch_a, pca_patch_b = torch.split(pca_patch, [half_channels, pca_channels - half_channels], dim=0)\n",
    "\n",
    "            superpixel_patch = extract_precise_superpixel_pca(self.superpixel_pca_dict, self.superpixel_labels, x, y)\n",
    "            superpixel_patch = torch.tensor(superpixel_patch[:, None, None], dtype=torch.float32)\n",
    "            superpixel_patch = superpixel_patch.expand(-1, self.patch_size, self.patch_size)\n",
    "\n",
    "            global_patch = extract_cube(self.global_pca, x, y, (self.patch_size, self.patch_size))\n",
    "            global_patch = torch.tensor(global_patch, dtype=torch.float32)\n",
    "\n",
    "            cube_a = torch.cat([pca_patch_a, superpixel_patch], dim=0)\n",
    "            cube_b = torch.cat([pca_patch_b, global_patch], dim=0)\n",
    "            return cube_a, cube_b\n",
    "\n",
    "\n",
    "def contrastive_loss_ce_hard_negatives(features_a, features_b, temperature=0.1, num_negatives=2):\n",
    "    \"\"\"\n",
    "    Cross-Entropy Contrastive Loss using hardest negative sampling.\n",
    "    - Each anchor (features_a[i]) uses its positive (features_b[i])\n",
    "    - Negatives are selected as least similar samples in [features_a; features_b]\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = features_a.size(0)\n",
    "\n",
    "    # Normalize\n",
    "    features_a = F.normalize(features_a, dim=1)\n",
    "    features_b = F.normalize(features_b, dim=1)\n",
    "\n",
    "    # Combine: total 2N candidates\n",
    "    all_features = torch.cat([features_a, features_b], dim=0)  # [2N, D]\n",
    "\n",
    "    # Cosine sim between anchors and all\n",
    "    sim_matrix = torch.matmul(features_a, all_features.T) / temperature  # [N, 2N]\n",
    "\n",
    "    # Mask out own positive (at i + batch_size)\n",
    "    pos_indices = torch.arange(batch_size, device=features_a.device)\n",
    "    sim_matrix[torch.arange(batch_size), pos_indices + batch_size] = float('-inf')\n",
    "\n",
    "    # Select top-k lowest similarity (hard negatives)\n",
    "    _, neg_indices = torch.topk(sim_matrix, k=num_negatives, dim=1, largest=False)  # [N, k]\n",
    "\n",
    "    # Construct new logits: [N, 1 + k] → positive + k negatives\n",
    "    pos_sim = torch.sum(features_a * features_b, dim=1, keepdim=True) / temperature  # [N, 1]\n",
    "    neg_sims = torch.gather(sim_matrix, 1, neg_indices)  # [N, k]\n",
    "    logits = torch.cat([pos_sim, neg_sims], dim=1)  # [N, 1+k]\n",
    "\n",
    "    # Labels: positive is index 0\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long, device=features_a.device)\n",
    "\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def compute_global_pca(hsi_data, merged_train_truth, num_components=20):\n",
    "    \"\"\"\n",
    "    仅在训练区域计算全局 PCA，并应用到整个 HSI。\n",
    "    如果训练样本数少于目标维度，则跳过 PCA，直接返回原始数据。\n",
    "    \n",
    "    参数:\n",
    "        hsi_data: numpy.ndarray, 形状为 (B, H, W)\n",
    "        merged_train_truth: coo_matrix, 标注的训练区域\n",
    "        num_components: int, PCA 降维维度\n",
    "        \n",
    "    返回:\n",
    "        global_pca_map: numpy.ndarray, 形状为 (num_components, H, W)\n",
    "    \"\"\"\n",
    "    c, h, w = hsi_data.shape\n",
    "    rows, cols = merged_train_truth.row, merged_train_truth.col\n",
    "    train_spectra = hsi_data[:, rows, cols].T  # shape: (N, C)\n",
    "    num_samples = train_spectra.shape[0]\n",
    "\n",
    "    if num_samples < num_components:\n",
    "        print(f\"样本数 {num_samples} 小于 PCA 维度 {num_components}，跳过 PCA，返回原始数据\")\n",
    "        return hsi_data  # 不降维\n",
    "\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(train_spectra)\n",
    "\n",
    "    reshaped_data = hsi_data.reshape(c, -1).T  # shape: (H*W, C)\n",
    "    reduced_data = pca.transform(reshaped_data)  # shape: (H*W, num_components)\n",
    "    global_pca_map = reduced_data.T.reshape(num_components, h, w)\n",
    "\n",
    "    return global_pca_map\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def apply_pca_on_candidate(hsi_data, candidate_truth, num_components=40, use_pca=True):\n",
    "    \"\"\"\n",
    "    在 candidate_truth 区域进行 PCA 或保留原始光谱，返回结果格式统一。\n",
    "    \n",
    "    返回：\n",
    "    - candidate_data: shape [C, H, W]，只在候选区域填值，其余为 0\n",
    "    - samples: shape [N, C]，候选像素的特征\n",
    "    - coords: List of (row, col)\n",
    "    - explained_variance_ratio 或 None\n",
    "    \"\"\"\n",
    "    c, h, w = hsi_data.shape\n",
    "    rows, cols = candidate_truth.row, candidate_truth.col\n",
    "    spectra = hsi_data[:, rows, cols].T  # shape: (N, C)\n",
    "    coords = list(zip(rows, cols))\n",
    "\n",
    "    if use_pca:\n",
    "        if num_components > c:\n",
    "            raise ValueError(f\"PCA 维度 {num_components} 不能大于原始通道数 {c}\")\n",
    "        pca = PCA(n_components=num_components)\n",
    "        reduced = pca.fit_transform(spectra)  # shape: (N, num_components)\n",
    "        result_c = num_components\n",
    "        final_data = reduced\n",
    "        var_ratio = pca.explained_variance_ratio_\n",
    "    else:\n",
    "        reduced = spectra\n",
    "        result_c = c\n",
    "        final_data = reduced\n",
    "        var_ratio = None\n",
    "\n",
    "    # 构建 [C, H, W] 格式，只填候选区域\n",
    "    candidate_data = np.zeros((result_c, h, w), dtype=np.float32)\n",
    "    for i, (r, c_) in enumerate(coords):\n",
    "        candidate_data[:, r, c_] = final_data[i]\n",
    "\n",
    "    return candidate_data, reduced, coords, var_ratio\n",
    "\n",
    "def prepare_data_from_loaded(mode, hsi_data, candidate_truth, train_truth, test_truth, info):\n",
    "    \"\"\"\n",
    "    使用你已加载的 pavia、truth 等数据，按模式准备训练用的数据。\n",
    "    根据模式控制是否对主数据、superpixel 和 global 特征降维。\n",
    "    \"\"\"\n",
    "    merged_truth = merge_train_test(train_truth, test_truth, (info[\"height\"], info[\"width\"]))\n",
    "\n",
    "    # === 模式配置 ===\n",
    "    use_pca = mode == \"original\"\n",
    "    use_raw = mode == \"local_global\"\n",
    "    main_channels = 40 if use_pca else hsi_data.shape[0]\n",
    "    side_channels = 20 if use_pca else hsi_data.shape[0]\n",
    "\n",
    "    # === 1. 主数据 ===\n",
    "    if use_raw:\n",
    "        main_data = hsi_data  # 不降维\n",
    "    else:\n",
    "        main_data, _, _, _ = apply_pca_on_candidate(\n",
    "            hsi_data, candidate_truth,\n",
    "            num_components=main_channels,\n",
    "            use_pca=use_pca\n",
    "        )\n",
    "\n",
    "    # === 2. 超像素标签 ===\n",
    "    superpixel_labels = superpixel_segmentation(hsi_data)\n",
    "\n",
    "    # === 3. Superpixel 特征 ===\n",
    "    if use_raw:\n",
    "        # 不做 PCA，直接用均值光谱\n",
    "        superpixel_pca_map, superpixel_pca_dict = compute_superpixel_feature_map(\n",
    "            hsi_data, superpixel_labels\n",
    "        )\n",
    "    else:\n",
    "        superpixel_pca_map, superpixel_pca_dict = compute_superpixel_pca(\n",
    "            hsi_data, superpixel_labels, candidate_truth, num_components=side_channels\n",
    "        )\n",
    "\n",
    "    # === 4. Global 特征 ===\n",
    "    if use_raw:\n",
    "        global_pca_map = hsi_data  # 不做 PCA\n",
    "    else:\n",
    "        global_pca_map = compute_global_pca(\n",
    "            hsi_data, candidate_truth, num_components=side_channels\n",
    "        )\n",
    "\n",
    "    return main_data, superpixel_pca_map, superpixel_pca_dict, superpixel_labels, global_pca_map\n",
    "\n",
    "def run_experiment(mode, dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    input_channels = dataset[0][0].shape[0]  # 获取 cube_a 的通道数\n",
    "    feature_extractor = FeatureExtractor(input_channels=input_channels).cuda()\n",
    "    projection_head = ProjectionHead(input_dim=128, output_dim=8).cuda()\n",
    "    optimizer = optim.Adam(list(feature_extractor.parameters()) + list(projection_head.parameters()), lr=1e-4)\n",
    "\n",
    "    os.makedirs('./pth', exist_ok=True)\n",
    "    num_epochs = 50\n",
    "    temperature = 1.0\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        feature_extractor.train()\n",
    "        projection_head.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for cube_a, cube_b in dataloader:\n",
    "            cube_a, cube_b = cube_a.cuda(), cube_b.cuda()\n",
    "            features_a = feature_extractor(cube_a)\n",
    "            features_b = feature_extractor(cube_b)\n",
    "            proj_a = projection_head(features_a)\n",
    "            proj_b = projection_head(features_b)\n",
    "\n",
    "            loss = contrastive_loss_ce_hard_negatives(proj_a, proj_b, temperature=1, num_negatives=10)\n",
    "            #loss = contrastive_loss(proj_a, proj_b, temperature=1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        loss_values.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Mode: {mode} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = f'final/model_{mode}.pth'\n",
    "    print(\"model:\", model_path)\n",
    "    torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'feature_extractor_state_dict': feature_extractor.state_dict(),\n",
    "        'projection_head_state_dict': projection_head.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss_values[-1],\n",
    "    }, model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    # 可选：画损失图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), loss_values, marker='o')\n",
    "    plt.title(f\"Loss Curve ({mode})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'pth/loss_curve_{mode}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模式名\t                  是否降维\t数据组合方式\n",
    "\"original\"\t             ✅\t     20pca + 20super vs 20pca + 20global\n",
    "\"local_global\"\t         ❌\t     super103 vs global103\n",
    "\"pca_super_rawglobal\"\t ❌\t     raw51 + super103 vs raw51 + global103\n",
    "\"raw_only\"\t             ❌\t     raw51 vs raw51（切半）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pavia University shape: (103, 610, 340)\n",
      "Train samples: 90\n",
      "Candidate samples: 3921\n",
      "Test samples: 42776\n",
      "Epoch [1/50] | Mode: raw_only | Loss: 1.2246\n",
      "Epoch [2/50] | Mode: raw_only | Loss: 0.9725\n",
      "Epoch [3/50] | Mode: raw_only | Loss: 0.9028\n",
      "Epoch [4/50] | Mode: raw_only | Loss: 0.8737\n",
      "Epoch [5/50] | Mode: raw_only | Loss: 0.8577\n",
      "Epoch [6/50] | Mode: raw_only | Loss: 0.8408\n",
      "Epoch [7/50] | Mode: raw_only | Loss: 0.8298\n",
      "Epoch [8/50] | Mode: raw_only | Loss: 0.8238\n",
      "Epoch [9/50] | Mode: raw_only | Loss: 0.8204\n",
      "Epoch [10/50] | Mode: raw_only | Loss: 0.8177\n",
      "Epoch [11/50] | Mode: raw_only | Loss: 0.8161\n",
      "Epoch [12/50] | Mode: raw_only | Loss: 0.8134\n",
      "Epoch [13/50] | Mode: raw_only | Loss: 0.8130\n",
      "Epoch [14/50] | Mode: raw_only | Loss: 0.8109\n",
      "Epoch [15/50] | Mode: raw_only | Loss: 0.8103\n",
      "Epoch [16/50] | Mode: raw_only | Loss: 0.8084\n",
      "Epoch [17/50] | Mode: raw_only | Loss: 0.8083\n",
      "Epoch [18/50] | Mode: raw_only | Loss: 0.8076\n",
      "Epoch [19/50] | Mode: raw_only | Loss: 0.8073\n",
      "Epoch [20/50] | Mode: raw_only | Loss: 0.8059\n",
      "Epoch [21/50] | Mode: raw_only | Loss: 0.8054\n",
      "Epoch [22/50] | Mode: raw_only | Loss: 0.8048\n",
      "Epoch [23/50] | Mode: raw_only | Loss: 0.8048\n",
      "Epoch [24/50] | Mode: raw_only | Loss: 0.8043\n",
      "Epoch [25/50] | Mode: raw_only | Loss: 0.8043\n",
      "Epoch [26/50] | Mode: raw_only | Loss: 0.8040\n",
      "Epoch [27/50] | Mode: raw_only | Loss: 0.8034\n",
      "Epoch [28/50] | Mode: raw_only | Loss: 0.8032\n",
      "Epoch [29/50] | Mode: raw_only | Loss: 0.8034\n",
      "Epoch [30/50] | Mode: raw_only | Loss: 0.8030\n",
      "Epoch [31/50] | Mode: raw_only | Loss: 0.8030\n",
      "Epoch [32/50] | Mode: raw_only | Loss: 0.8024\n",
      "Epoch [33/50] | Mode: raw_only | Loss: 0.8028\n",
      "Epoch [34/50] | Mode: raw_only | Loss: 0.8022\n",
      "Epoch [35/50] | Mode: raw_only | Loss: 0.8025\n",
      "Epoch [36/50] | Mode: raw_only | Loss: 0.8023\n",
      "Epoch [37/50] | Mode: raw_only | Loss: 0.8024\n",
      "Epoch [38/50] | Mode: raw_only | Loss: 0.8020\n",
      "Epoch [39/50] | Mode: raw_only | Loss: 0.8017\n",
      "Epoch [40/50] | Mode: raw_only | Loss: 0.8017\n",
      "Epoch [41/50] | Mode: raw_only | Loss: 0.8022\n",
      "Epoch [42/50] | Mode: raw_only | Loss: 0.8015\n",
      "Epoch [43/50] | Mode: raw_only | Loss: 0.8012\n",
      "Epoch [44/50] | Mode: raw_only | Loss: 0.8014\n",
      "Epoch [45/50] | Mode: raw_only | Loss: 0.8011\n",
      "Epoch [46/50] | Mode: raw_only | Loss: 0.8012\n",
      "Epoch [47/50] | Mode: raw_only | Loss: 0.8010\n",
      "Epoch [48/50] | Mode: raw_only | Loss: 0.8011\n",
      "Epoch [49/50] | Mode: raw_only | Loss: 0.8008\n",
      "Epoch [50/50] | Mode: raw_only | Loss: 0.8016\n",
      "model: final/model_raw_only.pth\n",
      "Model saved to: final/model_raw_only.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = Path(r\"E:\\code\\-\\对比学习\\fx\\otherdataset\")\n",
    "\n",
    "candidate_counts = {\n",
    "    1: 548, 2: 540, 3: 392,\n",
    "    4: 524, 5: 265, 6: 532,\n",
    "    7: 375, 8: 514, 9: 231\n",
    "}\n",
    "\n",
    "pavia, train_truth, candidate_truth, test_truth, info = load__with_full_test(\n",
    "    data_path,\n",
    "    candidate_counts=candidate_counts,\n",
    "    num_train_per_class=15\n",
    ")\n",
    "\n",
    "print(f\"Pavia University shape: {pavia.shape}\")\n",
    "print(f\"Train samples: {train_truth.count_nonzero()}\")         #  90\n",
    "print(f\"Candidate samples: {candidate_truth.count_nonzero()}\") #  3921\n",
    "print(f\"Test samples: {test_truth.count_nonzero()}\")           #  42776\n",
    "\n",
    "\n",
    "# 调用 prepare\n",
    "mode = \"raw_only\"  # 你想跑的模式\n",
    "main_data, superpixel_pca_map, superpixel_pca_dict, superpixel_labels, global_pca_map = prepare_data_from_loaded(\n",
    "    mode, pavia, candidate_truth, train_truth, test_truth, info\n",
    ")\n",
    "\n",
    "# 用于 S3PCADataset 构建数据集\n",
    "dataset = S3PCADataset(\n",
    "    main_data,\n",
    "    superpixel_pca_map,\n",
    "    superpixel_pca_dict,\n",
    "    superpixel_labels,\n",
    "    global_pca_map,\n",
    "    mode=mode\n",
    ")\n",
    "\n",
    "run_experiment(mode, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "# === 模式选择 ===\n",
    "mode = \"raw_only\"  # original, local_global, pca_super_rawglobal, raw_only\n",
    "\n",
    "# === 输入通道定义（匹配对比训练）===\n",
    "def get_input_channels_for_classification(mode):\n",
    "    if mode == \"original\":\n",
    "        return 20\n",
    "    elif mode == \"local_global\":\n",
    "        return 103\n",
    "    elif mode == \"pca_super_rawglobal\":\n",
    "        return 51 + 103\n",
    "    elif mode == \"raw_only\":\n",
    "        return 51\n",
    "    else:\n",
    "        raise ValueError(f\"未知模式: {mode}\")\n",
    "# === 从稀疏矩阵提取 label ===\n",
    "def extract_labels(truth, label_dict):\n",
    "    rows, cols, labels = truth.row, truth.col, truth.data\n",
    "    label_to_index = {label_value: idx for idx, label_value in enumerate(label_dict.keys())}\n",
    "    mapped_labels = [label_to_index[label] for label in labels if label in label_to_index]\n",
    "    return [(row, col, label) for row, col, label in zip(rows, cols, mapped_labels)]\n",
    "\n",
    "# === 分类数据集 ===\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, patch_size=11):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, label = self.labels[idx]\n",
    "        cube = extract_cube(self.data, x, y, (self.patch_size, self.patch_size))\n",
    "        cube = cube.astype(np.float32)\n",
    "\n",
    "        return torch.tensor(cube), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# === 特征提取器 ===\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# === 分类头 ===\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# === 分类训练函数 ===\n",
    "def train_classification_model(feature_extractor, classification_head, train_loader, optimizer, criterion, num_epochs=50):\n",
    "    feature_extractor.train()\n",
    "    classification_head.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for cubes, labels in train_loader:\n",
    "            cubes, labels = cubes.cuda(), labels.cuda()\n",
    "            features = feature_extractor(cubes)\n",
    "            outputs = classification_head(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        if epoch < 20:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def evaluate_classification_model_with_details(feature_extractor, classification_head, test_loader, num_classes):\n",
    "    \"\"\"\n",
    "    评估分类模型的性能，并输出详细预测结果和分布。\n",
    "\n",
    "    参数:\n",
    "        feature_extractor: 冻结的特征提取器\n",
    "        classification_head: 分类头\n",
    "        test_loader: 测试数据加载器\n",
    "        num_classes: 总类别数\n",
    "    \"\"\"\n",
    "    feature_extractor.eval()\n",
    "    classification_head.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 初始化预测计数，并移动到 GPU\n",
    "    prediction_counts = torch.zeros(num_classes, dtype=torch.int64).cuda()\n",
    "    class_correct = torch.zeros(num_classes, dtype=torch.int64).cuda()\n",
    "    class_total = torch.zeros(num_classes, dtype=torch.int64).cuda()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cubes, labels in test_loader:\n",
    "            cubes, labels = cubes.cuda(), labels.cuda()\n",
    "\n",
    "            # 提取特征并进行预测\n",
    "            features = feature_extractor(cubes)\n",
    "            outputs = classification_head(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # 更新统计信息\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            prediction_counts += torch.bincount(predicted, minlength=num_classes).cuda()\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                class_total[label] += 1\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "            # 保存详细信息\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Overall Accuracy\n",
    "    overall_accuracy = correct / total\n",
    "\n",
    "    # Average Accuracy (每个类别的平均准确率)\n",
    "    average_accuracy = (class_correct.float() / class_total.float()).mean().item()\n",
    "\n",
    "    # Per-class Accuracy\n",
    "    per_class_accuracy = class_correct.float() / class_total.float()\n",
    "\n",
    "    # Kappa 系数\n",
    "    kappa = cohen_kappa_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
    "    print(f\"Kappa Coefficient: {kappa:.4f}\")\n",
    "\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    for cls, count in enumerate(prediction_counts.cpu()):  # 将分布从 GPU 移回 CPU\n",
    "        print(f\"Class {cls}: {count} predictions\")\n",
    "\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for cls, acc in enumerate(per_class_accuracy):\n",
    "        print(f\"Class {cls}: {acc:.4f}\")\n",
    "    for cls, acc in enumerate(per_class_accuracy):\n",
    "        print(f\"{acc:.4f}\")\n",
    "    return overall_accuracy, average_accuracy, kappa, all_predictions, all_labels, per_class_accuracy.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def prepare_data_by_mode(mode, hsi_data, train_truth=None, test_truth=None, num_superpixel_components=51):\n",
    "    \"\"\"\n",
    "    根据 mode 返回 main_data:\n",
    "    - \"local_global\": 使用原始高光谱数据，不做降维\n",
    "    - \"raw_only\": 使用 PCA 全图降维\n",
    "    - \"pca_super_rawglobal\": 原始数据 + 局部 PCA（仅用有标签区域拟合）拼接\n",
    "    \"\"\"\n",
    "    print(f\"[prepare_data_by_mode] 当前模式: {mode}\")\n",
    "    C_raw, H, W = hsi_data.shape\n",
    "\n",
    "    if mode == \"original\":\n",
    "        print(\"使用原始高光谱数据，不做 PCA\")\n",
    "        return hsi_data\n",
    "\n",
    "    elif mode == \"raw_only\":\n",
    "        num_components = 51\n",
    "        reshaped = hsi_data.reshape(C_raw, -1).T  # shape: (H*W, C)\n",
    "        print(f\"对全图做 PCA 降维到 {num_components} 维...\")\n",
    "        pca = PCA(n_components=num_components, svd_solver='auto')\n",
    "        reduced = pca.fit_transform(reshaped).T  # shape: (num_components, H*W)\n",
    "        reduced_data = reduced.reshape(num_components, H, W)\n",
    "        print(f\"PCA 完成，降维后形状: {reduced_data.shape}\")\n",
    "        return reduced_data\n",
    "    elif mode == \"local_global\":\n",
    "        return hsi_data\n",
    "    elif mode == \"local_global1\":\n",
    "        if train_truth is None or test_truth is None:\n",
    "            raise ValueError(\"必须提供 train_truth 和 test_truth 用于构造局部 PCA 区域\")\n",
    "\n",
    "        # 1. 构建全图上的标签区域（合并 train 和 test）\n",
    "        merged_truth = merge_train_test(train_truth, test_truth, shape=(H, W))\n",
    "\n",
    "        # 2. 超像素分割（对整个图做）\n",
    "        superpixel_labels = superpixel_segmentation(hsi_data)\n",
    "\n",
    "        # 3. 在 merged truth 区域执行每个超像素的 PCA 拟合，输出整个图的特征图\n",
    "        superpixel_pca_map, _ = compute_superpixel_pca(\n",
    "            hsi_data, superpixel_labels, merged_truth,\n",
    "            num_components=num_superpixel_components\n",
    "        )\n",
    "\n",
    "        # 4. 拼接原始数据和 superpixel PCA 特征图\n",
    "        combined = np.concatenate([hsi_data, superpixel_pca_map], axis=0)\n",
    "        print(f\"[组合特征] 原始: {hsi_data.shape} + SuperPCA: {superpixel_pca_map.shape} → 总: {combined.shape}\")\n",
    "        return combined\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"暂未支持的 mode: {mode}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prepare_data_by_mode] 当前模式: raw_only\n",
      "对全图做 PCA 降维到 51 维...\n",
      "PCA 完成，降维后形状: (51, 610, 340)\n",
      "\n",
      "[调试] 当前模式: raw_only\n",
      "[调试] 输入数据 main_data.shape: (51, 610, 340) (channels, height, width)\n",
      "[调试] 实际输入通道数: 51\n",
      "[调试] 模式期望通道数: 51\n",
      "模型： final/model_raw_only.pth\n",
      "[调试] 可加载层数: 21 / 21\n",
      " 成功加载部分预训练参数（自动适配输入通道）\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: 准备分类输入数据 ===\n",
    "'''\n",
    "main_data = prepare_data_by_mode(\n",
    "    mode, pavia\n",
    ")\n",
    "'''\n",
    "main_data = prepare_data_by_mode(\n",
    "    mode=mode,\n",
    "    hsi_data=pavia,\n",
    "    train_truth=train_truth,\n",
    "    test_truth=test_truth,\n",
    "    num_superpixel_components=51  # 你可以调这个\n",
    ")\n",
    "\n",
    "print(f\"\\n[调试] 当前模式: {mode}\")\n",
    "print(f\"[调试] 输入数据 main_data.shape: {main_data.shape} (channels, height, width)\")\n",
    "\n",
    "train_labels = extract_labels(train_truth, info[\"label_dict\"])\n",
    "test_labels = extract_labels(test_truth, info[\"label_dict\"])\n",
    "patch_size = 11  # 可自定义\n",
    "\n",
    "train_dataset = ClassificationDataset(main_data, train_labels, patch_size=patch_size)\n",
    "test_dataset = ClassificationDataset(main_data, test_labels, patch_size=patch_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# === STEP 2: 模型准备 ===\n",
    "# 自动从 main_data 获取输入通道数\n",
    "actual_input_channels = main_data.shape[0]\n",
    "expected_input_channels = get_input_channels_for_classification(mode)\n",
    "\n",
    "print(f\"[调试] 实际输入通道数: {actual_input_channels}\")\n",
    "print(f\"[调试] 模式期望通道数: {expected_input_channels}\")\n",
    "if actual_input_channels != expected_input_channels:\n",
    "    print(\" 通道数不一致！已使用实际输入通道数进行初始化\")\n",
    "\n",
    "feature_extractor = FeatureExtractor(input_channels=actual_input_channels).cuda()\n",
    "\n",
    "checkpoint_path = f\"final/model_{mode}.pth\"\n",
    "print(\"模型：\",checkpoint_path)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "pretrained_state = checkpoint['feature_extractor_state_dict']\n",
    "current_state = feature_extractor.state_dict()\n",
    "\n",
    "# 只加载 shape 一致的层\n",
    "filtered_state = {\n",
    "    k: v for k, v in pretrained_state.items()\n",
    "    if k in current_state and v.shape == current_state[k].shape\n",
    "}\n",
    "\n",
    "print(f\"[调试] 可加载层数: {len(filtered_state)} / {len(pretrained_state)}\")\n",
    "\n",
    "# 更新模型参数并加载\n",
    "current_state.update(filtered_state)\n",
    "feature_extractor.load_state_dict(current_state)\n",
    "print(f\" 成功加载部分预训练参数（自动适配输入通道）\")\n",
    "\n",
    "# === 后续流程保持不变 ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Loss: 2.4146\n",
      "Epoch 2/200 | Loss: 1.9953\n",
      "Epoch 3/200 | Loss: 1.7216\n",
      "Epoch 4/200 | Loss: 1.5855\n",
      "Epoch 5/200 | Loss: 1.4691\n",
      "Epoch 6/200 | Loss: 1.3209\n",
      "Epoch 7/200 | Loss: 1.3413\n",
      "Epoch 8/200 | Loss: 1.2250\n",
      "Epoch 9/200 | Loss: 1.1484\n",
      "Epoch 10/200 | Loss: 1.0691\n",
      "Epoch 11/200 | Loss: 1.0428\n",
      "Epoch 12/200 | Loss: 0.9356\n",
      "Epoch 13/200 | Loss: 0.9080\n",
      "Epoch 14/200 | Loss: 0.8439\n",
      "Epoch 15/200 | Loss: 0.8150\n",
      "Epoch 16/200 | Loss: 0.7737\n",
      "Epoch 17/200 | Loss: 0.7337\n",
      "Epoch 18/200 | Loss: 0.7185\n",
      "Epoch 19/200 | Loss: 0.6480\n",
      "Epoch 20/200 | Loss: 0.6232\n",
      "Epoch 21/200 | Loss: 0.6020\n",
      "Epoch 22/200 | Loss: 0.5908\n",
      "Epoch 23/200 | Loss: 0.6332\n",
      "Epoch 24/200 | Loss: 0.5465\n",
      "Epoch 25/200 | Loss: 0.5530\n",
      "Epoch 26/200 | Loss: 0.5133\n",
      "Epoch 27/200 | Loss: 0.4852\n",
      "Epoch 28/200 | Loss: 0.4516\n",
      "Epoch 29/200 | Loss: 0.4099\n",
      "Epoch 30/200 | Loss: 0.4367\n",
      "Epoch 31/200 | Loss: 0.4004\n",
      "Epoch 32/200 | Loss: 0.3923\n",
      "Epoch 33/200 | Loss: 0.3569\n",
      "Epoch 34/200 | Loss: 0.3310\n",
      "Epoch 35/200 | Loss: 0.3267\n",
      "Epoch 36/200 | Loss: 0.3646\n",
      "Epoch 37/200 | Loss: 0.2800\n",
      "Epoch 38/200 | Loss: 0.2741\n",
      "Epoch 39/200 | Loss: 0.2457\n",
      "Epoch 40/200 | Loss: 0.2587\n",
      "Epoch 41/200 | Loss: 0.2524\n",
      "Epoch 42/200 | Loss: 0.2208\n",
      "Epoch 43/200 | Loss: 0.2121\n",
      "Epoch 44/200 | Loss: 0.2044\n",
      "Epoch 45/200 | Loss: 0.2254\n",
      "Epoch 46/200 | Loss: 0.1883\n",
      "Epoch 47/200 | Loss: 0.1703\n",
      "Epoch 48/200 | Loss: 0.2440\n",
      "Epoch 49/200 | Loss: 0.1634\n",
      "Epoch 50/200 | Loss: 0.1460\n",
      "Epoch 51/200 | Loss: 0.1533\n",
      "Epoch 52/200 | Loss: 0.1356\n",
      "Epoch 53/200 | Loss: 0.1477\n",
      "Epoch 54/200 | Loss: 0.1251\n",
      "Epoch 55/200 | Loss: 0.1100\n",
      "Epoch 56/200 | Loss: 0.1208\n",
      "Epoch 57/200 | Loss: 0.0961\n",
      "Epoch 58/200 | Loss: 0.1008\n",
      "Epoch 59/200 | Loss: 0.0980\n",
      "Epoch 60/200 | Loss: 0.1030\n",
      "Epoch 61/200 | Loss: 0.0977\n",
      "Epoch 62/200 | Loss: 0.1037\n",
      "Epoch 63/200 | Loss: 0.0792\n",
      "Epoch 64/200 | Loss: 0.0883\n",
      "Epoch 65/200 | Loss: 0.0756\n",
      "Epoch 66/200 | Loss: 0.1242\n",
      "Epoch 67/200 | Loss: 0.1042\n",
      "Epoch 68/200 | Loss: 0.0699\n",
      "Epoch 69/200 | Loss: 0.0656\n",
      "Epoch 70/200 | Loss: 0.0801\n",
      "Epoch 71/200 | Loss: 0.0842\n",
      "Epoch 72/200 | Loss: 0.0523\n",
      "Epoch 73/200 | Loss: 0.0582\n",
      "Epoch 74/200 | Loss: 0.0537\n",
      "Epoch 75/200 | Loss: 0.0545\n",
      "Epoch 76/200 | Loss: 0.0523\n",
      "Epoch 77/200 | Loss: 0.0461\n",
      "Epoch 78/200 | Loss: 0.0421\n",
      "Epoch 79/200 | Loss: 0.0535\n",
      "Epoch 80/200 | Loss: 0.0586\n",
      "Epoch 81/200 | Loss: 0.0464\n",
      "Epoch 82/200 | Loss: 0.0508\n",
      "Epoch 83/200 | Loss: 0.0533\n",
      "Epoch 84/200 | Loss: 0.0336\n",
      "Epoch 85/200 | Loss: 0.0307\n",
      "Epoch 86/200 | Loss: 0.0455\n",
      "Epoch 87/200 | Loss: 0.0536\n",
      "Epoch 88/200 | Loss: 0.0315\n",
      "Epoch 89/200 | Loss: 0.0286\n",
      "Epoch 90/200 | Loss: 0.0327\n",
      "Epoch 91/200 | Loss: 0.0307\n",
      "Epoch 92/200 | Loss: 0.0312\n",
      "Epoch 93/200 | Loss: 0.0263\n",
      "Epoch 94/200 | Loss: 0.0846\n",
      "Epoch 95/200 | Loss: 0.0292\n",
      "Epoch 96/200 | Loss: 0.0253\n",
      "Epoch 97/200 | Loss: 0.0285\n",
      "Epoch 98/200 | Loss: 0.0858\n",
      "Epoch 99/200 | Loss: 0.0274\n",
      "Epoch 100/200 | Loss: 0.0222\n",
      "Epoch 101/200 | Loss: 0.0208\n",
      "Epoch 102/200 | Loss: 0.0268\n",
      "Epoch 103/200 | Loss: 0.0222\n",
      "Epoch 104/200 | Loss: 0.0302\n",
      "Epoch 105/200 | Loss: 0.0209\n",
      "Epoch 106/200 | Loss: 0.0227\n",
      "Epoch 107/200 | Loss: 0.0185\n",
      "Epoch 108/200 | Loss: 0.0167\n",
      "Epoch 109/200 | Loss: 0.0182\n",
      "Epoch 110/200 | Loss: 0.0161\n",
      "Epoch 111/200 | Loss: 0.0148\n",
      "Epoch 112/200 | Loss: 0.0169\n",
      "Epoch 113/200 | Loss: 0.0161\n",
      "Epoch 114/200 | Loss: 0.0167\n",
      "Epoch 115/200 | Loss: 0.0206\n",
      "Epoch 116/200 | Loss: 0.0132\n",
      "Epoch 117/200 | Loss: 0.0142\n",
      "Epoch 118/200 | Loss: 0.0164\n",
      "Epoch 119/200 | Loss: 0.0185\n",
      "Epoch 120/200 | Loss: 0.0139\n",
      "Epoch 121/200 | Loss: 0.0157\n",
      "Epoch 122/200 | Loss: 0.0166\n",
      "Epoch 123/200 | Loss: 0.0123\n",
      "Epoch 124/200 | Loss: 0.0168\n",
      "Epoch 125/200 | Loss: 0.0125\n",
      "Epoch 126/200 | Loss: 0.0126\n",
      "Epoch 127/200 | Loss: 0.0143\n",
      "Epoch 128/200 | Loss: 0.0127\n",
      "Epoch 129/200 | Loss: 0.0135\n",
      "Epoch 130/200 | Loss: 0.0191\n",
      "Epoch 131/200 | Loss: 0.0203\n",
      "Epoch 132/200 | Loss: 0.0107\n",
      "Epoch 133/200 | Loss: 0.0237\n",
      "Epoch 134/200 | Loss: 0.0122\n",
      "Epoch 135/200 | Loss: 0.0093\n",
      "Epoch 136/200 | Loss: 0.0123\n",
      "Epoch 137/200 | Loss: 0.0097\n",
      "Epoch 138/200 | Loss: 0.0166\n",
      "Epoch 139/200 | Loss: 0.0102\n",
      "Epoch 140/200 | Loss: 0.0176\n",
      "Epoch 141/200 | Loss: 0.0119\n",
      "Epoch 142/200 | Loss: 0.0170\n",
      "Epoch 143/200 | Loss: 0.0105\n",
      "Epoch 144/200 | Loss: 0.0095\n",
      "Epoch 145/200 | Loss: 0.0089\n",
      "Epoch 146/200 | Loss: 0.0093\n",
      "Epoch 147/200 | Loss: 0.0090\n",
      "Epoch 148/200 | Loss: 0.0101\n",
      "Epoch 149/200 | Loss: 0.0156\n",
      "Epoch 150/200 | Loss: 0.0141\n",
      "Epoch 151/200 | Loss: 0.0082\n",
      "Epoch 152/200 | Loss: 0.0116\n",
      "Epoch 153/200 | Loss: 0.0068\n",
      "Epoch 154/200 | Loss: 0.0065\n",
      "Epoch 155/200 | Loss: 0.0080\n",
      "Epoch 156/200 | Loss: 0.0083\n",
      "Epoch 157/200 | Loss: 0.0091\n",
      "Epoch 158/200 | Loss: 0.0079\n",
      "Epoch 159/200 | Loss: 0.0080\n",
      "Epoch 160/200 | Loss: 0.0124\n",
      "Epoch 161/200 | Loss: 0.0085\n",
      "Epoch 162/200 | Loss: 0.0088\n",
      "Epoch 163/200 | Loss: 0.0082\n",
      "Epoch 164/200 | Loss: 0.0084\n",
      "Epoch 165/200 | Loss: 0.0081\n",
      "Epoch 166/200 | Loss: 0.0056\n",
      "Epoch 167/200 | Loss: 0.0055\n",
      "Epoch 168/200 | Loss: 0.0110\n",
      "Epoch 169/200 | Loss: 0.0057\n",
      "Epoch 170/200 | Loss: 0.0072\n",
      "Epoch 171/200 | Loss: 0.0090\n",
      "Epoch 172/200 | Loss: 0.0072\n",
      "Epoch 173/200 | Loss: 0.0052\n",
      "Epoch 174/200 | Loss: 0.0058\n",
      "Epoch 175/200 | Loss: 0.0056\n",
      "Epoch 176/200 | Loss: 0.0109\n",
      "Epoch 177/200 | Loss: 0.0064\n",
      "Epoch 178/200 | Loss: 0.0083\n",
      "Epoch 179/200 | Loss: 0.0057\n",
      "Epoch 180/200 | Loss: 0.0050\n",
      "Epoch 181/200 | Loss: 0.0051\n",
      "Epoch 182/200 | Loss: 0.0078\n",
      "Epoch 183/200 | Loss: 0.0071\n",
      "Epoch 184/200 | Loss: 0.0069\n",
      "Epoch 185/200 | Loss: 0.0040\n",
      "Epoch 186/200 | Loss: 0.0061\n",
      "Epoch 187/200 | Loss: 0.0043\n",
      "Epoch 188/200 | Loss: 0.0094\n",
      "Epoch 189/200 | Loss: 0.0062\n",
      "Epoch 190/200 | Loss: 0.0107\n",
      "Epoch 191/200 | Loss: 0.0055\n",
      "Epoch 192/200 | Loss: 0.0047\n",
      "Epoch 193/200 | Loss: 0.0054\n",
      "Epoch 194/200 | Loss: 0.0045\n",
      "Epoch 195/200 | Loss: 0.0067\n",
      "Epoch 196/200 | Loss: 0.0077\n",
      "Epoch 197/200 | Loss: 0.0053\n",
      "Epoch 198/200 | Loss: 0.0040\n",
      "Epoch 199/200 | Loss: 0.0049\n",
      "Epoch 200/200 | Loss: 0.0046\n",
      "Overall Accuracy: 0.7941\n",
      "Average Accuracy: 0.8486\n",
      "Kappa Coefficient: 0.7406\n",
      "\n",
      "Prediction Distribution:\n",
      "Class 0: 5351 predictions\n",
      "Class 1: 13876 predictions\n",
      "Class 2: 2780 predictions\n",
      "Class 3: 3232 predictions\n",
      "Class 4: 1368 predictions\n",
      "Class 5: 9642 predictions\n",
      "Class 6: 1898 predictions\n",
      "Class 7: 3600 predictions\n",
      "Class 8: 1029 predictions\n",
      "\n",
      "Per-class Accuracy:\n",
      "Class 0: 0.7349\n",
      "Class 1: 0.7410\n",
      "Class 2: 0.7465\n",
      "Class 3: 0.9586\n",
      "Class 4: 0.9866\n",
      "Class 5: 0.9763\n",
      "Class 6: 0.8323\n",
      "Class 7: 0.6782\n",
      "Class 8: 0.9831\n",
      "0.7349\n",
      "0.7410\n",
      "0.7465\n",
      "0.9586\n",
      "0.9866\n",
      "0.9763\n",
      "0.8323\n",
      "0.6782\n",
      "0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7940901440059847,\n",
       " 0.848615288734436,\n",
       " 0.7406423915781228,\n",
       " [2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " array([0.73488164, 0.7410049 , 0.746546  , 0.95855093, 0.9866171 ,\n",
       "        0.97633725, 0.8323308 , 0.67816406, 0.9831045 ], dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "classification_head = ClassificationHead(128, num_classes=len(info[\"label_dict\"])).cuda()\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\": feature_extractor.parameters(), \"lr\": 1e-4},\n",
    "    {\"params\": classification_head.parameters(), \"lr\": 1e-3},\n",
    "])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# === STEP 3: 训练 ===\n",
    "train_classification_model(feature_extractor, classification_head, train_loader, optimizer, criterion, num_epochs=200)\n",
    "\n",
    "# === STEP 4: 评估 ===\n",
    "evaluate_classification_model_with_details(feature_extractor, classification_head, test_loader, num_classes=len(info[\"label_dict\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(info[\"label_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "local global\n",
    "Overall Accuracy: 0.7881\n",
    "Average Accuracy: 0.8242\n",
    "Kappa Coefficient: 0.7287\n",
    "\n",
    "Prediction Distribution:\n",
    "Class 0: 5362 predictions\n",
    "Class 1: 15654 predictions\n",
    "Class 2: 3483 predictions\n",
    "Class 3: 3267 predictions\n",
    "Class 4: 1353 predictions\n",
    "Class 5: 7359 predictions\n",
    "Class 6: 1621 predictions\n",
    "Class 7: 3698 predictions\n",
    "Class 8: 979 predictions\n",
    "\n",
    "Per-class Accuracy:\n",
    "Class 0: 0.7406\n",
    "Class 1: 0.7971\n",
    "Class 2: 0.7423\n",
    "Class 3: 0.9566\n",
    "Class 4: 0.9896\n",
    "Class 5: 0.7578\n",
    "Class 6: 0.8301\n",
    "Class 7: 0.6143\n",
    "Class 8: 0.9894"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
