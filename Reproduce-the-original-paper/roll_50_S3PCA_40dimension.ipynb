{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import skimage.exposure\n",
    "import warnings\n",
    "from io import StringIO\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.segmentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _read_roi(path: Path, shape) -> coo_matrix:\n",
    "    \"\"\"\n",
    "    读取 ENVI 软件导出 ROI 文件的 txt 文件，生成一个稀疏矩阵，表示每个像素点的类别标签。\n",
    "    \n",
    "    :param path: 文件路径，ENVI ROI 文件的路径\n",
    "    :param shape: 图像的形状（height, width）\n",
    "    :return: 一个稀疏矩阵，非零值表示像素点的类别标签\n",
    "    \"\"\"\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)  # 忽略 loadtxt 的警告\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    current_label = 0\n",
    "    buffer = \"\"\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            # 判断是否为新 ROI 的分割点\n",
    "            if line.strip() == \"\" or line.startswith(\";\") or \"ROI\" in line:\n",
    "                if buffer:  # 如果缓冲区有内容，解析为坐标数据\n",
    "                    roi_data = np.loadtxt(StringIO(buffer), usecols=(2, 1), dtype=int)\n",
    "                    if roi_data.size > 0:\n",
    "                        r, c = roi_data.T\n",
    "                        rows.extend(r)\n",
    "                        cols.extend(c)\n",
    "                        data.extend([current_label] * len(r))\n",
    "                    buffer = \"\"  # 清空缓冲区\n",
    "                # 如果遇到 ROI name 行，增加类别标签\n",
    "                if \"ROI name\" in line:\n",
    "                    current_label += 1\n",
    "            else:\n",
    "                buffer += line  # 将数据加入缓冲区\n",
    "\n",
    "        # 处理最后一个 ROI\n",
    "        if buffer:\n",
    "            roi_data = np.loadtxt(StringIO(buffer), usecols=(2, 1), dtype=int)\n",
    "            if roi_data.size > 0:\n",
    "                r, c = roi_data.T\n",
    "                rows.extend(r)\n",
    "                cols.extend(c)\n",
    "                data.extend([current_label] * len(r))\n",
    "\n",
    "    warnings.resetwarnings()\n",
    "\n",
    "    # 创建稀疏矩阵\n",
    "    img = coo_matrix((data, (rows, cols)), shape=shape, dtype=int)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_houston2013(data_path: Path):\n",
    "    \"\"\"\n",
    "    从本地路径加载 Houston2013 数据集。\n",
    "\n",
    "    :param data_path: 数据集的根目录路径。\n",
    "    :return: (casi, lidar, train_truth, test_truth, info)\n",
    "    \"\"\"\n",
    "    FILES_PATH = data_path\n",
    "    assert FILES_PATH.exists(), f\"{FILES_PATH} does not exist. Please check the path.\"\n",
    "\n",
    "    # 加载图像数据\n",
    "    lidar = skimage.io.imread(FILES_PATH / '2013_IEEE_GRSS_DF_Contest_LiDAR.tif')[np.newaxis, :, :]  # (1, 349, 1905)\n",
    "    casi = skimage.io.imread(FILES_PATH / '2013_IEEE_GRSS_DF_Contest_CASI.tif').transpose(2, 0, 1)   # (144, 349, 1905)\n",
    "\n",
    "    # 加载训练集和测试集的真值\n",
    "    train_truth = _read_roi(FILES_PATH / '2013_IEEE_GRSS_DF_Contest_Samples_TR.txt', (349, 1905))\n",
    "    test_truth = _read_roi(FILES_PATH / '2013_IEEE_GRSS_DF_Contest_Samples_VA.txt', (349, 1905))\n",
    "\n",
    "    # 数据集元信息\n",
    "    info = {\n",
    "        'n_band_casi': 144,\n",
    "        'n_band_lidar': 1,\n",
    "        'width': 1905,\n",
    "        'height': 349,\n",
    "        'label_dict': {\n",
    "            1: 'Healthy grass',\n",
    "            2: 'Stressed grass',\n",
    "            3: 'Synthetic grass',\n",
    "            4: 'Trees',\n",
    "            5: 'Soil',\n",
    "            6: 'Water',\n",
    "            7: 'Residential',\n",
    "            8: 'Commercial',\n",
    "            9: 'Road',\n",
    "            10: 'Highway',\n",
    "            11: 'Railway',\n",
    "            12: 'Parking Lot 1',\n",
    "            13: 'Parking Lot 2',\n",
    "            14: 'Tennis Court',\n",
    "            15: 'Running Track',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return casi, lidar, train_truth, test_truth, info\n",
    "\n",
    "def merge_train_test(train_truth, test_truth, shape):\n",
    "    \"\"\"\n",
    "    合并训练集和测试集稀疏矩阵为一个新的训练集矩阵。\n",
    "    \n",
    "    参数:\n",
    "        train_truth: coo_matrix, 原始训练集稀疏矩阵\n",
    "        test_truth: coo_matrix, 原始测试集稀疏矩阵\n",
    "        shape: tuple, 数据的形状 (height, width)\n",
    "        \n",
    "    返回:\n",
    "        merged_truth: coo_matrix, 合并后的训练集稀疏矩阵\n",
    "    \"\"\"\n",
    "    # 合并行、列和数据\n",
    "    merged_rows = np.concatenate([train_truth.row, test_truth.row])\n",
    "    merged_cols = np.concatenate([train_truth.col, test_truth.col])\n",
    "    merged_data = np.concatenate([train_truth.data, test_truth.data])\n",
    "    \n",
    "    # 创建新的稀疏矩阵\n",
    "    merged_truth = coo_matrix((merged_data, (merged_rows, merged_cols)), shape=shape)\n",
    "    return merged_truth\n",
    "\n",
    "def apply_pca_train_only(hsi_data, train_truth, num_components=40):\n",
    "    \"\"\"\n",
    "    使用训练区域的光谱数据训练 PCA 模型，并应用到整个数据集。\n",
    "    \n",
    "    参数:\n",
    "        hsi_data: numpy.ndarray, 高光谱图像数据, 形状为 [C, H, W]\n",
    "        train_truth: coo_array, 训练区域的稀疏矩阵，表示训练样本的位置\n",
    "        num_components: int, 保留的主成分数量\n",
    "        \n",
    "    返回:\n",
    "        pca_data: numpy.ndarray, PCA 降维后的数据，形状为 [num_components, H, W]\n",
    "        explained_variance_ratio: PCA 的累计解释方差比\n",
    "    \"\"\"\n",
    "    c, h, w = hsi_data.shape  # 高光谱数据的形状\n",
    "    rows, cols = train_truth.row, train_truth.col  # 提取训练区域的行列索引\n",
    "    \n",
    "    # 提取训练区域的光谱数据 [num_samples, num_channels]\n",
    "    train_spectra = hsi_data[:, rows, cols].T  # 转置为 [num_samples, num_channels]\n",
    "    \n",
    "    # 在训练区域数据上拟合 PCA\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(train_spectra)  # 仅在训练区域数据上训练 PCA\n",
    "    \n",
    "    # 转换整个数据集 [C, H, W] -> [H×W, C]\n",
    "    reshaped_data = hsi_data.reshape(c, -1).T  # [H×W, C]\n",
    "    reduced_data = pca.transform(reshaped_data)  # 降维 [H×W, num_components]\n",
    "    \n",
    "    # 恢复为原始图像的形状 [num_components, H, W]\n",
    "    pca_data = reduced_data.T.reshape(num_components, h, w)  # [num_components, H, W]\n",
    "    \n",
    "    return pca_data, pca.explained_variance_ratio_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<tifffile.TiffPage 0 @2949772> parsing GDAL_NODATA tag raised ValueError('-3.4028234663852886e+38 is not castable to float32')\n"
     ]
    }
   ],
   "source": [
    "# 训练集路径\n",
    "data_path = Path(r\"E:\\code\\-\\对比学习\\fx\\Houston2013\\2013_DFTC\")\n",
    "\n",
    "# 加载数据集\n",
    "casi, lidar, train_truth, test_truth, info = load_houston2013(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(truth, label_dict):\n",
    "    \"\"\"\n",
    "    从稀疏矩阵中提取标注数据，格式为 [(row, col, label), ...]。\n",
    "    并将标签值映射到 [0, len(label_dict)-1] 的范围。\n",
    "    \"\"\"\n",
    "    rows, cols, labels = truth.row, truth.col, truth.data\n",
    "    # 创建从标签值到索引的映射\n",
    "    label_to_index = {label_value: idx for idx, label_value in enumerate(label_dict.keys())}\n",
    "    mapped_labels = [label_to_index[label] for label in labels if label in label_to_index]\n",
    "    return [(row, col, label) for row, col, label in zip(rows, cols, mapped_labels)]\n",
    "\n",
    "\n",
    "# 提取立方块函数\n",
    "def extract_cube(data, x, y, size):\n",
    "    \"\"\"\n",
    "    从高光谱图像中提取一个立方块，并在边缘不足时进行填充。\n",
    "    参数:\n",
    "        data: 高光谱数据, 形状为 [C, H, W]\n",
    "        x, y: 中心像素的坐标\n",
    "        size: 立方块的大小 (s, s)\n",
    "    返回:\n",
    "        cube: 提取的立方块, 形状为 [C, s, s]\n",
    "    \"\"\"\n",
    "    c, h, w = data.shape\n",
    "    half_size = size[0] // 2\n",
    "    x_min = max(0, x - half_size)\n",
    "    x_max = min(h, x + half_size + 1)\n",
    "    y_min = max(0, y - half_size)\n",
    "    y_max = min(w, y + half_size + 1)\n",
    "    \n",
    "    cube = data[:, x_min:x_max, y_min:y_max]\n",
    "\n",
    "    # 对称填充，确保形状为 (C, s, s)\n",
    "    pad_width = [\n",
    "        (0, 0),  # 不填充通道维度\n",
    "        (max(0, half_size - x), max(0, x + half_size + 1 - h)),  # 高度填充\n",
    "        (max(0, half_size - y), max(0, y + half_size + 1 - w)),  # 宽度填充\n",
    "    ]\n",
    "    cube = np.pad(cube, pad_width, mode=\"reflect\")\n",
    "    return cube\n",
    "\n",
    "\n",
    "# PCA 降维函数\n",
    "def apply_pca_train_only(hsi_data, train_truth, num_components=20):\n",
    "    \"\"\"\n",
    "    使用训练区域的光谱数据训练 PCA 模型，并应用到整个数据集。\n",
    "    参数:\n",
    "        hsi_data: 高光谱数据, 形状为 [C, H, W]\n",
    "        train_truth: coo_array, 训练区域的稀疏矩阵，表示训练样本的位置\n",
    "        num_components: 保留的主成分数量\n",
    "    返回:\n",
    "        pca_data: 降维后的数据, 形状为 [num_components, H, W]\n",
    "        explained_variance_ratio: PCA 的累计解释方差比\n",
    "    \"\"\"\n",
    "    c, h, w = hsi_data.shape\n",
    "    rows, cols = train_truth.row, train_truth.col  # 提取训练区域的行列索引\n",
    "\n",
    "    # 提取训练区域的光谱数据 [num_samples, num_channels]\n",
    "    train_spectra = hsi_data[:, rows, cols].T  # 转置为 [num_samples, num_channels]\n",
    "\n",
    "    # 在训练区域数据上拟合 PCA\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(train_spectra)  # 仅在训练区域数据上训练 PCA\n",
    "\n",
    "    # 转换整个数据集\n",
    "    reshaped_data = hsi_data.reshape(c, -1).T  # [H×W, C]\n",
    "    reduced_data = pca.transform(reshaped_data)  # 降维 [H×W, num_components]\n",
    "\n",
    "    # 恢复为原始图像的形状\n",
    "    pca_data = reduced_data.T.reshape(num_components, h, w)  # [num_components, H, W]\n",
    "    \n",
    "    return pca_data, pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "# 分类数据集定义\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, data, labels, patch_size=11):\n",
    "        \"\"\"\n",
    "        构造分类数据集。\n",
    "        参数:\n",
    "            data: PCA 降维后的数据 [C, H, W]\n",
    "            labels: [(row, col, label), ...]，标注的样本\n",
    "            patch_size: 立方块大小 (patch_size, patch_size)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, label = self.labels[idx]\n",
    "        cube = extract_cube(self.data, x, y, (self.patch_size, self.patch_size))  # 提取立方块\n",
    "        cube_tensor = torch.tensor(cube).float()  # 转换为浮点张量\n",
    "        return cube_tensor, label\n",
    "\n",
    "\n",
    "# 候选集生成函数\n",
    "def create_candidate_dataset(train_labels, num_classes, samples_per_class):\n",
    "    \"\"\"\n",
    "    从训练样本中按每类抽取指定数量的样本，构造候选集。\n",
    "    参数:\n",
    "        train_labels: 原始训练样本 [(row, col, label), ...]\n",
    "        num_classes: 类别数量\n",
    "        samples_per_class: 每个类别抽取的样本数量\n",
    "    返回:\n",
    "        candidate_labels: 候选样本 [(row, col, label), ...]\n",
    "    \"\"\"\n",
    "    class_samples = defaultdict(list)\n",
    "    for sample in train_labels:\n",
    "        class_samples[sample[2]].append(sample)\n",
    "\n",
    "    candidate_labels = []\n",
    "    for cls in range(num_classes):\n",
    "        if cls in class_samples:\n",
    "            samples = class_samples[cls]\n",
    "            if len(samples) < samples_per_class:\n",
    "                print(f\"Warning: Class {cls} has only {len(samples)} samples, less than {samples_per_class}.\")\n",
    "            candidate_labels.extend(random.sample(samples, min(samples_per_class, len(samples))))\n",
    "        else:\n",
    "            print(f\"Warning: Class {cls} has no samples in training set.\")\n",
    "    return candidate_labels\n",
    "\n",
    "\n",
    "# 定义特征提取器\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        return x.view(x.size(0), -1)  # Flatten to [batch_size, features]\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(train_truth.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Overall Accuracy: 0.8369\n",
      "Mean Average Accuracy: 0.8556\n",
      "Mean Kappa Coefficient: 0.8230\n",
      "Best Seed: 0 with Overall Accuracy: 0.8767\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# 加载模型的代码\n",
    "checkpoint_path = \"pth/S3PCA_merge_v1.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "if 'feature_extractor_state_dict' not in checkpoint:\n",
    "    raise KeyError(\"Checkpoint does not contain 'feature_extractor_state_dict'. Please check the file.\")\n",
    "\n",
    "def evaluate_classification_model_with_details(feature_extractor, classification_head, test_loader, num_classes):\n",
    "    feature_extractor.eval()\n",
    "    classification_head.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    prediction_counts = torch.zeros(num_classes, dtype=torch.int64).cuda()\n",
    "    class_correct = torch.zeros(num_classes, dtype=torch.int64).cuda()\n",
    "    class_total = torch.zeros(num_classes, dtype=torch.int64).cuda()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cubes, labels in test_loader:\n",
    "            cubes, labels = cubes.cuda(), labels.cuda()\n",
    "            features = feature_extractor(cubes)\n",
    "            outputs = classification_head(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            prediction_counts += torch.bincount(predicted, minlength=num_classes).cuda()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                class_total[label] += 1\n",
    "                if label == pred:\n",
    "                    class_correct[label] += 1\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    overall_accuracy = correct / total\n",
    "    average_accuracy = (class_correct.float() / class_total.float()).mean().item()\n",
    "    kappa = cohen_kappa_score(all_labels, all_predictions)\n",
    "    return overall_accuracy, average_accuracy, kappa, all_predictions, all_labels\n",
    "\n",
    "def train_and_evaluate_with_seeds(test_truth, casi, info, num_classes, num_seeds=50, num_epochs=200):\n",
    "    overall_accuracies = []\n",
    "    average_accuracies = []\n",
    "    kappa_coefficients = []\n",
    "    best_accuracy = 0\n",
    "    best_seed = None\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # 重新初始化特征提取器并加载预训练权重\n",
    "        feature_extractor = FeatureExtractor(input_channels=40).cuda()\n",
    "        feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n",
    "        feature_extractor.train()\n",
    "        \n",
    "        test_labels = extract_labels(test_truth, info['label_dict'])\n",
    "        ptest_data, explained_variance_ratio = apply_pca_train_only(casi, test_truth, num_components=40)\n",
    "        test_dataset = ClassificationDataset(ptest_data, test_labels, patch_size=11)\n",
    "        candidate_labels = create_candidate_dataset(test_labels, num_classes=len(info['label_dict']), samples_per_class=10)\n",
    "        candidate_dataset = ClassificationDataset(ptest_data, candidate_labels, patch_size=11)\n",
    "        \n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        candidate_loader = DataLoader(candidate_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        classification_head = ClassificationHead(input_dim=128, num_classes=num_classes).cuda()\n",
    "        optimizer = optim.Adam([\n",
    "            {\"params\": feature_extractor.parameters(), \"lr\": 1e-4},\n",
    "            {\"params\": classification_head.parameters(), \"lr\": 1e-3},\n",
    "        ])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        classification_head.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for cubes, labels in candidate_loader:\n",
    "                cubes, labels = cubes.cuda(), labels.cuda()\n",
    "                features = feature_extractor(cubes)\n",
    "                outputs = classification_head(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        overall_accuracy, average_accuracy, kappa, _, _ = evaluate_classification_model_with_details(\n",
    "            feature_extractor, classification_head, test_loader, num_classes\n",
    "        )\n",
    "        \n",
    "        overall_accuracies.append(overall_accuracy)\n",
    "        average_accuracies.append(average_accuracy)\n",
    "        kappa_coefficients.append(kappa)\n",
    "        \n",
    "        if overall_accuracy > best_accuracy:\n",
    "            best_accuracy = overall_accuracy\n",
    "            best_seed = seed\n",
    "    \n",
    "    mean_overall_accuracy = np.mean(overall_accuracies)\n",
    "    mean_average_accuracy = np.mean(average_accuracies)\n",
    "    mean_kappa = np.mean(kappa_coefficients)\n",
    "    \n",
    "    print(f\"Mean Overall Accuracy: {mean_overall_accuracy:.4f}\")\n",
    "    print(f\"Mean Average Accuracy: {mean_average_accuracy:.4f}\")\n",
    "    print(f\"Mean Kappa Coefficient: {mean_kappa:.4f}\")\n",
    "    print(f\"Best Seed: {best_seed} with Overall Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "train_and_evaluate_with_seeds(test_truth, casi, info, num_classes=num_classes, num_seeds=50, num_epochs=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
